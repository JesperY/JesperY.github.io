---
title: 'AI中的决策论（变分法的使用）'
date: 2025-02-13T10:52:44+08:00
draft: false
ShowToc: true
TocOpen: true
tags:
  - machine learning
---



## 线性回归前瞻

在机器学习中，我们认为模型在做回归时实际上学到了一种分布。例如我们给定一些样本，他们来自函数 $y=sin(x)+\epsilon$，其中误差项遵循正态分布 $N(\mu,\sigma)$ ，当模型根据这些数据进行回归时，他会自然的计算出对于每个输入 $x$ 其对应的预测输出 $y=p(t|x)$。也就是说模型首先给出的并不是一个确定的值，而是围绕真实值在内的一个概率分布，然后才能确定哪个值最接近真实值。

## 决策论

根据上述我们就发现模型的行为被分为了两部分，学习概率分布，根据分布决定输出值。

学习概率分布可以由极大似然完成，但是如何决定最终的输出值呢？或者说当我们得到了输出的概率分布$P(t|x)$后，如何确定一个 $f$ 令其从概率分布中选择要输出的值呢？

毫无疑问我们要选择最接近真实值 $t$ 的输出，因此我们可以构建一个损失函数 $L(t,f(x))$ 来衡量预测值和真实值之间的差距。但我们在进行预测时并不知道真实值到底是什么，我们只知道其概率分布 $P(t|x)$，因此我们不能直接最小化损失函数本身，而是要最小化期望损失，公式如下：
$$
E[L]=\int\int L(t,f(x))p(x,t)dxdt
$$
我们需要指定一个损失函数，假如我们使用平方差作为损失函数，则公式变化如下：
$$
E[L]=\int\int \{f(x)-t\}^2p(x,t)dxdt
$$
我们需要找到一个令期望损失最小的函数 $f$ 来决定输出，而上式是一个泛函数，因此我们采用变分法，对 $f$ 求导得如下公式：
$$
\frac{\partial E[L]}{\partial f}=2\int \{f(x)-t\}p(x,t)dt=0
$$
令求导后的公式等于零，则可以找到令期望损失最小的 $f$ 。使用概率论的加法和乘法定理化简后得到：
$$
f(x)=\int tp(t|x)dt=E[t|x]
$$
对于服从正态分布的 $t$ 而言，其均值就可以直接从学习到的概率分布中得到。

## 变分法

什么是变分法？

变分法是一种求极值的方法，但是其应用于泛函数。

所谓泛函数就是函数的函数，当一个公式，例如上面的第一个公式，将一个未知的函数映射到另一个实数，则这个公式就是一个泛函数。如果我们的目的是找出这个泛函数的极值，则可以对其中的未知函数求偏导，并令偏导结果等于零，类似于初等函数求极值的过程，从而找到我们需要的未知函数所满足的条件。
