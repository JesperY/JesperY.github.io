---
title: '最小二乘'
date: 2025-02-15T14:55:29+08:00
draft: false
ShowToc: true
TocOpen: true
tags:
  - math
---

## 最小二乘

最小二乘法是参数估计中的一种方法，例如在做线性回归时，我们通常会构建一个误差函数，用来衡量回归模型的预测值和真实值之间的误差，通过最小化这个误差构建最优回归模型。而当我们选择残差平方和作为误差函数时，最小化这个误差函数的过程就是最小二乘法。

所谓最小二乘中的二乘，其实就是平方的意思。例如我们拟合一个函数 $\hat y=f(x)$ ，则他的残差平方和公式如下：
$$
Loss=\sum_{i=1}^n(y_i-\hat y_i)^2
$$
此时 $Loss$ 是一个关于 $f(x)$ 的参数的公式。

最小二乘法通常用于线性回归中，但是对于某些非线性问题，例如多项式模型，虽然关于自变量 $x$ 是非线性的，但是其关于参数的误差函数是线性的，所以也可以求解。即使参数也不是线性的，也可以通过梯度下降等方法进行收敛，只是无法直接得到解析解。

## 与极大似然的关系

极大似然通常用于求解概率分布的参数，即在给定一些观测值的情况下，找到令已有的观测值出现的概率最大的参数值。

此处不再赘述似然函数的具体形式。

极大似然也用于回归问题，对于回归模型 $y_i=x_i^T\beta+\epsilon_i,\epsilon_i\in\mathcal N(0,\sigma^2)$ ，这里定义了噪声符合正态分布，实际上噪声也可能遵循其他分布，此处使用正态分布是因为最小二乘实际上是正态分布下的极大似然的等价形式。该回归模型的对数似然如下：
$$
lnL(\beta,\sigma^2)=-\frac{n}{2}ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-x_i^T\beta)^2
$$
此时我们求解 $\beta$ 项时，极大似然过程就等价于最小化误差平方和过程。
